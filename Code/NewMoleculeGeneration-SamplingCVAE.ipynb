{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import CVAE\n",
    "from utils import *\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Descriptors import ExactMolWt\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "from rdkit.Chem.rdMolDescriptors import CalcTPSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 34\n",
    "iteration =10\n",
    "latent_size =300\n",
    "unit_size =512\n",
    "n_rnn_layer=3\n",
    "seq_length=185\n",
    "mean=0.0\n",
    "stddev=1.0\n",
    "num_prop=329\n",
    "save_file = \"NewSmilesWeirdLoss300LS300Epochs/model_300.ckpt-300\"\n",
    "target_prop = \"3000\"\n",
    "prop_file = \"smiles_new_dti.txt\"\n",
    "result_filename  = \"result0.8Loss300LS.txt\"\n",
    "lr=0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, char, vocab, _, _ = load_data(prop_file,seq_length)\n",
    "vocab_size = len(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import tensorflow as tf2 \n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "import threading\n",
    "from utils import *\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.Descriptors import ExactMolWt\n",
    "from rdkit.Chem.Crippen import MolLogP\n",
    "from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
    "\n",
    "\n",
    "class CVAE2():\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 char,\n",
    "                 b_s,\n",
    "                 l_s,\n",
    "                 l_r,\n",
    "                 num_prop,\n",
    "                 stddev,\n",
    "                 mean,\n",
    "                 u_s,\n",
    "                 n_rnn_layer\n",
    "                  ):\n",
    "        self.char = char\n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = b_s\n",
    "        self.latent_size = l_s\n",
    "        self.lr = tf.Variable(l_r, trainable=False)\n",
    "        self.num_prop = num_prop\n",
    "        self.stddev = stddev\n",
    "        self.mean = mean\n",
    "        self.unit_size = u_s\n",
    "        self.n_rnn_layer = n_rnn_layer\n",
    "        \n",
    "        self._create_network()\n",
    "\n",
    "\n",
    "    def _create_network(self):\n",
    "        self.X = tf.placeholder(tf.int32, [self.batch_size, None])\n",
    "        self.Y = tf.placeholder(tf.int32, [self.batch_size, None])\n",
    "        self.C = tf.placeholder(tf.float32, [self.batch_size, self.num_prop])\n",
    "        self.L = tf.placeholder(tf.int32, [self.batch_size])\n",
    "        \n",
    "\n",
    "        \n",
    "        decoded_rnn_size = [self.unit_size for i in range(self.n_rnn_layer)]\n",
    "        encoded_rnn_size = [self.unit_size for i in range(self.n_rnn_layer)]\n",
    "        \n",
    "        with tf.variable_scope('decode'):\n",
    "            decode_cell=[]\n",
    "            for i in decoded_rnn_size[:]:\n",
    "                decode_cell.append(tf.nn.rnn_cell.LSTMCell(i))\n",
    "            self.decoder = tf.nn.rnn_cell.MultiRNNCell(decode_cell)\n",
    "        \n",
    "        with tf.variable_scope('encode'):\n",
    "            encode_cell=[]\n",
    "            for i in encoded_rnn_size[:]:\n",
    "                encode_cell.append(tf.nn.rnn_cell.LSTMCell(i))\n",
    "            self.encoder = tf.nn.rnn_cell.MultiRNNCell(encode_cell)\n",
    "        \n",
    "        self.encoder_cell = tf.nn.rnn_cell.LSTMCell(self.unit_size)\n",
    "\n",
    "        self.weights = {}\n",
    "        self.biases = {}\n",
    "        self.eps = {\n",
    "            'eps' : tf.random_normal([self.batch_size, self.latent_size], stddev=self.stddev, mean=self.mean)\n",
    "        }\n",
    "\n",
    "\n",
    "        self.weights['softmax'] = tf.get_variable(\"softmaxw\", initializer=tf.random_uniform(shape=[decoded_rnn_size[-1], self.vocab_size], minval = -0.1, maxval = 0.1))       \n",
    "        \n",
    "        self.biases['softmax'] =  tf.get_variable(\"softmaxb\", initializer=tf.zeros(shape=[self.vocab_size]))\n",
    "        self.weights['out_mean'] = tf.get_variable(\"outmeanw\", initializer=tf2.initializers.GlorotUniform(), shape=[self.unit_size, self.latent_size]),\n",
    "        self.weights['out_log_sigma'] = tf.get_variable(\"outlogsigmaw\", initializer=tf2.initializers.GlorotUniform(), shape=[self.unit_size, self.latent_size]),\n",
    "        self.biases['out_mean'] = tf.get_variable(\"outmeanb\", initializer=tf2.initializers.GlorotUniform(), shape=[self.latent_size]),\n",
    "        self.biases['out_log_sigma'] = tf.get_variable(\"outlogsigmab\", initializer=tf2.initializers.GlorotUniform(), shape=[self.latent_size]),\n",
    "\n",
    "        self.embedding_encode = tf.get_variable(name = 'encode_embedding', shape = [self.latent_size, self.vocab_size], initializer = tf.random_uniform_initializer( minval = -0.1, maxval = 0.1))\n",
    "        \n",
    "        self.embedding = tf.nn.embedding_lookup(self.embedding_encode, self.X)\n",
    "\n",
    "        \n",
    "        self.latent_vector, self.mean, self.log_sigma = self.encode()\n",
    "        \n",
    "        self.decoded, decoded_logits = self.decode(self.latent_vector)\n",
    "        self.mol_pred = tf.argmax(self.decoded, axis=2)\n",
    "\n",
    "        #self.Y_generated = self.generate()\n",
    "\n",
    "        weights = tf.sequence_mask(self.L, tf.shape(self.X)[1])\n",
    "        weights = tf.cast(weights, tf.int32)\n",
    "        weights = tf.cast(weights, tf.float32)\n",
    "\n",
    "        self.reconstr_loss = tf.reduce_mean(tfa.seq2seq.sequence_loss(\n",
    "            logits=decoded_logits, targets=self.Y, weights=weights))\n",
    "        self.latent_loss = self.cal_latent_loss(self.mean, self.log_sigma)\n",
    "        self.real_loss =  tf.py_func(self.convert_to_smiles_tf, inp=[self.mol_pred, list(self.char.keys()), self.reconstr_loss, self.latent_loss, self.Y], Tout=tf.float32)\n",
    "        #npa = tf.make_ndarray(real_molecule)\n",
    "        \n",
    "       \n",
    "\n",
    "        # Loss\n",
    "\n",
    "        self.loss = self.latent_loss + self.reconstr_loss + self.real_loss\n",
    "        #self.loss = self.reconstr_loss \n",
    "        optimizer = tf.train.AdamOptimizer(self.lr)\n",
    "        self.opt = optimizer.minimize(self.loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "        self.saver = tf.train.Saver(max_to_keep=None)\n",
    "        #tf.train.start_queue_runners(sess=self.sess)\n",
    "\n",
    "\n",
    "        print (\"Network Ready\")\n",
    "\n",
    "    def encode(self): \n",
    "        X = tf.nn.embedding_lookup(self.embedding_encode, self.X)\n",
    "        C = tf.expand_dims(self.C, 1)\n",
    "        C = tf.tile(C, [1, tf.shape(X)[1], 1])\n",
    "        inp = tf.concat([X, C], axis=-1)\n",
    "\n",
    "        \n",
    "        _, state = tf.nn.dynamic_rnn(self.encoder, inp, dtype=tf.float32, scope = 'encode', sequence_length = self.L)\n",
    "        c,h = state[-1]\n",
    "        self.weights['out_mean'] = tf.reshape(self.weights['out_mean'], [self.unit_size, -1])\n",
    "        self.weights['out_log_sigma'] = tf.reshape(self.weights['out_log_sigma'], [self.unit_size, -1])\n",
    "        mean = tf.matmul(h, self.weights['out_mean'])+self.biases['out_mean']\n",
    "        log_sigma = tf.matmul(h, self.weights['out_log_sigma'])+self.biases['out_log_sigma']\n",
    "        \n",
    "\n",
    "        \n",
    "        retval = mean+tf.exp(log_sigma/2.0)*self.eps['eps']\n",
    "        print(\"latent space shape\", tf.shape(retval))\n",
    "        return retval, mean, log_sigma\n",
    "\n",
    "    def decode(self, Z):\n",
    "        seq_length=tf.shape(self.X)[1]\n",
    "        print(\"seq length\", seq_length)\n",
    "        new_Z = tf.tile(tf.expand_dims(Z, 1), [1, seq_length, 1])\n",
    "        C = tf.expand_dims(self.C, 1)\n",
    "        C = tf.tile(C, [1, tf.shape(self.X)[1], 1])\n",
    "        X = tf.nn.embedding_lookup(self.embedding_encode, self.X)\n",
    "        print(\"X\",  tf.shape(X))\n",
    "        inputs = tf.concat([new_Z, X, C], axis=-1)\n",
    "        print(\"inputs\", tf.shape(inputs))\n",
    "        \n",
    "        self.initial_decoded_state = tuple([tf.nn.rnn_cell.LSTMStateTuple(tf.zeros((self.batch_size, self.unit_size)), tf.zeros((self.batch_size, self.unit_size))) for i in range(self.n_rnn_layer)])\n",
    "        #self.initial_decoded_state=self.decoder.zero_state() \n",
    "        Y, self.output_decoded_state = tf.nn.dynamic_rnn(self.decoder, inputs, dtype=tf.float32, scope = 'decode', sequence_length = self.L, initial_state=self.initial_decoded_state)\n",
    "\n",
    "        Y = tf.reshape(Y, [self.batch_size*seq_length, -1])\n",
    "        Y = tf.matmul(Y, self.weights['softmax'])+self.biases['softmax']\n",
    "        Y_logits = tf.reshape(Y, [self.batch_size, seq_length, -1])\n",
    "        Y = tf.nn.softmax(Y_logits)\n",
    "        print(\"Y:\", Y, \"and logits:\", Y_logits)\n",
    "        return Y, Y_logits\n",
    "\n",
    "    def save(self, ckpt_path, global_step):\n",
    "        self.saver.save(self.sess, ckpt_path, global_step = global_step)\n",
    "        #print(\"model saved to '%s'\" % (ckpt_path))\n",
    "\n",
    "    def assign_lr(self, learning_rate):\n",
    "        self.sess.run(tf.assign(self.lr, learning_rate ))\n",
    "    \n",
    "    def restore(self, ckpt_path):\n",
    "        self.saver.restore(self.sess, ckpt_path)\n",
    "\n",
    "    def get_latent_vector(self, x, c, l):\n",
    "        return self.sess.run(self.latent_vector, feed_dict={self.X : x, self.C : c, self.L : l})\n",
    "\n",
    "    def cal_latent_loss(self, mean, log_sigma):\n",
    "        latent_loss = tf.reduce_mean(-0.5*(1+log_sigma-tf.square(mean)-tf.exp(log_sigma)))\n",
    "        return latent_loss\n",
    "    \n",
    "    def train(self, x, y, l, c):\n",
    "        _, loss, embedding = self.sess.run([self.opt, self.loss, self.embedding], feed_dict = {self.X :x, self.Y:y, self.L : l, self.C : c})\n",
    "        return loss, embedding\n",
    "    \n",
    "    def test(self, x, y, l, c):\n",
    "        mol_pred, loss  = self.sess.run([self.mol_pred, self.loss], feed_dict = {self.X :x, self.Y:y, self.L : l, self.C : c})\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    def convert_to_smiles_tf(self, vector, char, re_loss, la_loss, target):\n",
    "        list_char = char\n",
    "        #print(list_char)\n",
    "        #list_char = char.tolist()\n",
    "        \n",
    "        vector = vector.astype(int)\n",
    "        errors = []\n",
    "        new_string = \"\"\n",
    "\n",
    "        for molecule in range(len(vector)):\n",
    "            drug = []\n",
    "            y = []\n",
    "            for letter in range(len(vector[molecule])):\n",
    "                drug.append(list_char[vector[molecule][letter]].decode('utf-8'))\n",
    "                y.append(list_char[target[molecule][letter]].decode('utf-8'))\n",
    "\n",
    "            drugS = \"\".join(drug)\n",
    "            y = \"\".join(y)\n",
    "            #print(\"FINAL DRUG\", drugS)\n",
    "            print(\"FINAL DRUG\", drugS)\n",
    "            print(\"ACTUAL DRUG\", y)\n",
    "            index = drugS.find('E')\n",
    "            if(index == -1):\n",
    "                index = 186\n",
    "            real_molecule = Chem.MolFromSmiles(drugS[0:index])\n",
    "            real_loss = 0.8\n",
    "            if real_molecule is not None:\n",
    "                real_loss = 0.0\n",
    "            errors.append(real_loss)\n",
    "        print(\"Reconstruction loss\", re_loss)\n",
    "        print(\"Latent loss\", la_loss)\n",
    "        print(\"Real loss\", np.mean(errors) + 0.035*self.numIncBrackets('(', ')', drugS[0:index]) + 0.035*self.numWrongRing(drugS[0:index]))\n",
    "        return np.float32(np.mean(errors) + 0.035*self.numIncBrackets('(', ')', drugS[0:index]) + 0.035*self.numWrongRing(drugS[0:index]))\n",
    "\n",
    "    def numIncBrackets(self, charS, charE, string):\n",
    "        openC= 0\n",
    "        count =0\n",
    "\n",
    "        for i in range(len(string)):\n",
    "            if string[i]==charS:\n",
    "                openC += 1\n",
    "            elif string[i]==charE:\n",
    "                openC -= 1\n",
    "            if(openC<0):\n",
    "                count += 1\n",
    "                openC += 1\n",
    "        return count+openC\n",
    "    \n",
    "    def numWrongRing(self, string):\n",
    "        sum = 0\n",
    "        for number in range(20):\n",
    "            count= 0\n",
    "            for i in range(len(string)):\n",
    "                if string[i]==number:\n",
    "                    count += 1\n",
    "            if(count != 0 and count==1):\n",
    "                sum += (count%2)\n",
    "            elif(count != 0 and count > 1):\n",
    "                sum += (count%2 + (count - 2))\n",
    "        return sum\n",
    "    \n",
    "\n",
    "\n",
    "    def sample(self, latent_vector, c, start_codon, seq_length):\n",
    "        l = np.ones((self.batch_size)).astype(np.int32)\n",
    "        x=start_codon\n",
    "        preds = []\n",
    "        for i in range(seq_length):\n",
    "            if i==0:\n",
    "                x, state = self.sess.run([self.mol_pred, self.output_decoded_state], feed_dict = {self.X:x, self.latent_vector:latent_vector, self.L : l, self.C : c})\n",
    "            else:\n",
    "                x, state = self.sess.run([self.mol_pred, self.output_decoded_state], feed_dict = {self.X:x, self.latent_vector:latent_vector, self.L : l, self.C : c, self.initial_decoded_state:state})\n",
    "            preds.append(x)\n",
    "        return np.concatenate(preds,1).astype(int).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CVAE2(vocab_size, vocab, batch_size, latent_size, lr, num_prop, stddev, mean, unit_size,n_rnn_layer)\n",
    "model.restore(save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of parameters : ', np.sum([np.prod(v.shape) for v in tf.trainable_variables()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying the condition vector; which is the drug target interaction profile showing which proteins you want the drug to bid to \n",
    "target_dtis = np.array([[float(p) for p in \"0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t1\t1\t1\t0\t1\t1\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t0\t1\t0\".split()] for _ in range(batch_size)])\n",
    "print(target_prop)\n",
    "start_codon = np.array([np.array(list(map(vocab.get, 'X')))for _ in range(batch_size)])\n",
    "#print(start_codon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = []\n",
    "for _ in range(iteration):\n",
    "    latent_vector = s = np.random.normal(mean, stddev, (batch_size, latent_size))\n",
    "    generated = model.sample(latent_vector, target_dtis, start_codon, seq_length)\n",
    "    smiles += [convert_to_smiles(generated[i], char) for i in range(len(generated))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applytab(row):\n",
    "    return '\\t'.join(map(str,row.values))\n",
    "#print('\\t'.join(map(str,df.columns))) # to print the column names if requiredstr =\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('number of trial : ', len(smiles))\n",
    "smiles = list(set([s.split('E')[0] for s in smiles]    ))\n",
    "print ('number of generate smiles (after remove duplicated ones) : ', len(smiles))\n",
    "ms = [Chem.MolFromSmiles(s) for s in smiles]\n",
    "ms2 = [m for m in ms if m is not None]\n",
    "print ('number of valid smiles : ', len(ms2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filename  = \"results.txt\"\n",
    "\n",
    "with open(result_filename, 'w') as w:\n",
    "    \n",
    "    w.write('smiles\\tMW\\tLogP\\tTPSA\\n')\n",
    "    for m in ms:\n",
    "        #print(m)\n",
    "        if m is not None:\n",
    "            w.write(str(m)+\"\\n\")\n",
    "    w.close()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gudi",
   "language": "python",
   "name": "gudi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
